{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyZKB3fE_0_P"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_core langchain langchain_cohere langchain-community langchain-chroma duckduckgo-search pypdf langgraph langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "cohere_api_key = \"xxxxxxxxxxxxxx\"\n",
        "llm = Cohere(model=\"command-r-plus\", max_tokens=1024, temperature=0.2 , cohere_api_key = cohere_api_key )\n",
        "\n",
        "def Retriever(folder_path,chunk_size=1200, chunk_overlap=200):\n",
        "    loader = PyPDFDirectoryLoader(folder_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size , chunk_overlap=chunk_overlap)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
        "\n",
        "    db = Chroma.from_documents(chunks, embeddings)\n",
        "    return db.as_retriever()\n",
        "\n",
        "retriever = Retriever(file_name=\"your_data_source-folder\",chunk_size=1200, chunk_overlap=200)\n"
      ],
      "metadata": {
        "id": "1YBejD4fARTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icpSLX80rNYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Self Rag ?\"\n",
        "retriever.invoke(query)"
      ],
      "metadata": {
        "id": "i6QtNuPPBD4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from typing import Annotated, Dict, TypedDict\n",
        "from langchain_core.messages import BaseMessage"
      ],
      "metadata": {
        "id": "l_32WDRwEZH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpA4930BFZ0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "\n",
        "    keys: Dict[str, any]"
      ],
      "metadata": {
        "id": "WS0QZdEmEPuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Nodes ###\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
      ],
      "metadata": {
        "id": "lMGa0a6nEg7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    # Prompt\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "    }"
      ],
      "metadata": {
        "id": "qNBEaeXaEtzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK RELEVANCE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
        "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "    # Score\n",
        "    filtered_docs = []\n",
        "    relevant_count = 0\n",
        "    for d in documents:\n",
        "        score = chain.invoke(\n",
        "            {\n",
        "                \"question\": question,\n",
        "                \"context\": d.page_content,\n",
        "            }\n",
        "        )\n",
        "        grade = score[\"score\"]\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "            relevant_count += 1\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "\n",
        "    relevance_ratio = relevant_count / len(documents)\n",
        "    search = \"Yes\" if relevance_ratio < 0.6 else \"No\"\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\n",
        "            \"documents\": filtered_docs,\n",
        "            \"question\": question,\n",
        "            \"run_web_search\": search,\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "xrE_XMtiE4L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    # Create a prompt template with format instructions and the query\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
        "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
        "        Here is the initial question:\n",
        "        \\n ------- \\n\n",
        "        {question}\n",
        "        \\n ------- \\n\n",
        "        Provide an improved question without any premable, only respond with the updated question: \"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    # Prompt\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    better_question = chain.invoke({\"question\": question})\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": better_question}\n",
        "    }"
      ],
      "metadata": {
        "id": "qPfvF4KrFBPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question using Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Web results appended to documents.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    duckduckgo_search = DuckDuckGoSearchRun()\n",
        "    docs = duckduckgo_search.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join(docs)\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents,\"question\": question}}"
      ],
      "metadata": {
        "id": "RsYFCLi9FHTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer or re-generate a question for web search.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current state of the agent, including all keys.\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---DECIDE TO GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    filtered_documents = state_dict[\"documents\"]\n",
        "    search = state_dict[\"run_web_search\"]\n",
        "\n",
        "    if search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ],
      "metadata": {
        "id": "DF25AmOjEQ3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "H5a5dC6JFs0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"what is all about ?\"}\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value['keys']['generation'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Po7mt_cGF28",
        "outputId": "1cc4451b-11c1-44cf-f005-d6310d03b4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('SELF-RAG is a framework that enhances the quality and factual accuracy of '\n",
            " 'language models by combining retrieval and self-reflection. It trains a '\n",
            " 'single LM to retrieve relevant passages on demand and reflect on its output '\n",
            " 'to improve overall quality. This approach addresses the limitations of '\n",
            " 'conventional Retrieval-Augmented Generation (RAG).')\n"
          ]
        }
      ]
    }
  ]
}