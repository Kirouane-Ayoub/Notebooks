{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ham2jJwwCcEa",
        "outputId": "9ffe2d04-0ee7-4794-deb1-b4e9b1b15d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: llama-index-postprocessor-cohere-rerank\n",
            "Successfully installed llama-index-postprocessor-cohere-rerank-0.1.7\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index  llama-index-llms-cohere llama-index-embeddings-cohere llama-index-graph-stores-neo4j llama-index-postprocessor-cohere-rerank"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "tuwK9RHlF3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.cohere import CohereEmbedding\n",
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "\n",
        "cohere_api_key = \"xxxxxxxxxxxxx\"\n",
        "\n",
        "llm = Cohere(api_key=cohere_api_key, model=\"command-r-plus\")\n",
        "\n",
        "# with input_typ='search_query'\n",
        "embed_model = CohereEmbedding(\n",
        "    api_key=cohere_api_key,\n",
        "    model_name=\"embed-english-light-v3.0\",\n",
        "    input_type=\"search_query\",\n",
        ")\n",
        "# global\n",
        "Settings.embed_model = embed_model\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "b5-UeUC1DPQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAtjUPBfDlgh",
        "outputId": "bba042e9-f913-4819-96bf-7cceadb93071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-16 15:28:45--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham_es 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-16 15:28:45 (3.32 MB/s) - ‘data/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ],
      "metadata": {
        "id": "VcnTU6NlDv33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "JBrAcOIRDyS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
        "\n",
        "graph_store = Neo4jPropertyGraphStore(\n",
        "    username=\"neo4j\",\n",
        "    password=\"xxxxxxxxxxx\",\n",
        "    url=\"neo4j+s://xxxx.xxx.neo4j.io\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjJMfupXD2e6",
        "outputId": "8d223fce-faa3-4f5d-d2b8-461583b71060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PropertyGraphIndex\n",
        "\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        "    property_graph_store=graph_store,\n",
        "    show_progress=True,\n",
        ")"
      ],
      "metadata": {
        "id": "hGWnJF9EEY2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import (\n",
        "    CustomPGRetriever,\n",
        "    VectorContextRetriever,\n",
        "    TextToCypherRetriever,\n",
        ")\n",
        "from llama_index.core.graph_stores import PropertyGraphStore\n",
        "from llama_index.core.vector_stores.types import VectorStore\n",
        "from llama_index.core.embeddings import BaseEmbedding\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.llms import LLM\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "\n",
        "from typing import Optional, Any, Union\n",
        "\n",
        "\n",
        "class CustomRetriever(CustomPGRetriever):\n",
        "    \"\"\"Custom retriever with cohere reranking.\"\"\"\n",
        "\n",
        "    def init(\n",
        "        self,\n",
        "        ## vector context retriever params\n",
        "        embed_model: Optional[BaseEmbedding] = None,\n",
        "        vector_store: Optional[VectorStore] = None,\n",
        "        similarity_top_k: int = 4,\n",
        "        path_depth: int = 1,\n",
        "        ## text-to-cypher params\n",
        "        llm: Optional[LLM] = None,\n",
        "        text_to_cypher_template: Optional[Union[PromptTemplate, str]] = None,\n",
        "        ## cohere reranker params\n",
        "        cohere_api_key: Optional[str] = None,\n",
        "        cohere_top_n: int = 2,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n",
        "\n",
        "        self.vector_retriever = VectorContextRetriever(\n",
        "            self.graph_store,\n",
        "            include_text=self.include_text,\n",
        "            embed_model=embed_model,\n",
        "            vector_store=vector_store,\n",
        "            similarity_top_k=similarity_top_k,\n",
        "            path_depth=path_depth,\n",
        "        )\n",
        "\n",
        "        self.cypher_retriever = TextToCypherRetriever(\n",
        "            self.graph_store,\n",
        "            llm=llm,\n",
        "            text_to_cypher_template=text_to_cypher_template\n",
        "            ## NOTE: you can attach other parameters here if you'd like\n",
        "        )\n",
        "\n",
        "        self.reranker = CohereRerank(\n",
        "            api_key=cohere_api_key, top_n=cohere_top_n\n",
        "        )\n",
        "\n",
        "    def custom_retrieve(self, query_str: str) -> str:\n",
        "        \"\"\"Define custom retriever with reranking.\n",
        "\n",
        "        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n",
        "        \"\"\"\n",
        "        nodes_1 = self.vector_retriever.retrieve(query_str)\n",
        "        nodes_2 = self.cypher_retriever.retrieve(query_str)\n",
        "        reranked_nodes = self.reranker.postprocess_nodes(\n",
        "            nodes_1 + nodes_2, query_str=query_str\n",
        "        )\n",
        "\n",
        "        ## TMP: please change\n",
        "        final_text = \"\\n\\n\".join(\n",
        "            [n.get_content(metadata_mode=\"llm\") for n in reranked_nodes]\n",
        "        )\n",
        "\n",
        "        return final_text"
      ],
      "metadata": {
        "id": "4voXnUTEOTo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_sub_retriever = CustomRetriever(\n",
        "    index.property_graph_store,\n",
        "    include_text=True,\n",
        "    vector_store=index.vector_store,\n",
        "    cohere_api_key=\"YOUR COHERE API KEY\",\n",
        ")"
      ],
      "metadata": {
        "id": "fpQC1rJhOjJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n",
        ")\n",
        "response = query_engine.query(\"What did author do at Interleaf?\")\n"
      ],
      "metadata": {
        "id": "mUrnuXBROs3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}