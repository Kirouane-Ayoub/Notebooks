{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets sentencepiece transformers torch tqdm pandas"
      ],
      "metadata": {
        "id": "Ag7FT284LYCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "VwXLO6-NXAtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from transformers import LlamaTokenizerFast\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "g1wekKb999lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizerFast\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Function to get training data\n",
        "def get_training_corpus():\n",
        "    dataset = load_dataset(\"text\", data_files={\"train\": \"/content/cleaned_data.txt\"})\n",
        "    for i in range(0, len(dataset[\"train\"]), 1000):\n",
        "        yield dataset[\"train\"][i : i + 1000][\"text\"]\n",
        "\n",
        "# Initialize the base tokenizer\n",
        "base_tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
        "\n",
        "# Train the new tokenizer\n",
        "new_tokenizer = base_tokenizer.train_new_from_iterator(get_training_corpus(), vocab_size=1000)\n",
        "\n",
        "# Save the new tokenizer\n",
        "new_tokenizer.save_pretrained(\"new_tokenizer\")\n",
        "\n",
        "# Test the new tokenizer\n",
        "test_text = \"I was there that night \"\n",
        "encoded = new_tokenizer.encode(test_text)\n",
        "decoded = new_tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")"
      ],
      "metadata": {
        "id": "nhR3bwK-LX8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxVouPZWJ5zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelArgs:\n",
        "    def __init__(self,\n",
        "                 dim=4096,\n",
        "                 n_layers=32,\n",
        "                 n_heads=32,\n",
        "                 n_kv_heads=None,\n",
        "                 vocab_size=-1,\n",
        "                 multiple_of=256,\n",
        "                 ffn_dim_multiplier=None,\n",
        "                 norm_eps=1e-5,\n",
        "                 mode='train',\n",
        "                 batch_size=32,\n",
        "                 max_seq_length=32,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu', pad_token_id=None):\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.vocab_size = vocab_size\n",
        "        self.multiple_of = multiple_of\n",
        "        self.ffn_dim_multiplier = ffn_dim_multiplier\n",
        "        self.norm_eps = norm_eps\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.device = device\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "class TrainArgs(ModelArgs):\n",
        "    def __init__(self, n_epochs=10,\n",
        "                 log_interval=12,\n",
        "                 eval_iters=200,\n",
        "                 lr=3e-4,\n",
        "                 warmup_steps=4000,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_epochs = n_epochs\n",
        "        self.log_interval = log_interval\n",
        "        self.lr = lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.norm(2, dim=-1, keepdim=True)\n",
        "        return self.scale * x / torch.sqrt(norm ** 2 + self.eps)\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(dim_in, dim_out)\n",
        "        self.linear2 = nn.Linear(dim_in, dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.silu(self.linear1(x)) * self.linear2(x)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, seq_len, device):\n",
        "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, n_kv_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, dim = x.size()\n",
        "        q = self.query(x).view(batch_size, seq_length, self.n_heads, dim // self.n_heads)\n",
        "        k = self.key(x).view(batch_size, seq_length, self.n_kv_heads, dim // self.n_kv_heads)\n",
        "        v = self.value(x).view(batch_size, seq_length, self.n_kv_heads, dim // self.n_kv_heads)\n",
        "\n",
        "        scores = torch.einsum('bhqd, bhkd -> bhqk', q, k) / math.sqrt(dim // self.n_heads)\n",
        "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        context = torch.einsum('bhqk, bhvd -> bhqd', attn, v)\n",
        "        context = context.contiguous().view(batch_size, seq_length, dim)\n",
        "        return self.out(context)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.attn = GroupedQueryAttention(args.dim, args.n_heads, args.n_kv_heads)\n",
        "        self.norm1 = RMSNorm(args.dim, args.norm_eps)\n",
        "        self.norm2 = RMSNorm(args.dim, args.norm_eps)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(args.dim, args.ffn_dim_multiplier * args.dim),\n",
        "            SwiGLU(args.ffn_dim_multiplier * args.dim, args.dim)\n",
        "        )\n",
        "        self.rotary_emb = RotaryEmbedding(args.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len, device = x.shape[1], x.device\n",
        "        rotary_emb = self.rotary_emb(seq_len, device)  # Should match x.shape[2]\n",
        "        x = x + rotary_emb\n",
        "\n",
        "        attn_out = self.attn(self.norm1(x))\n",
        "        x = x + attn_out\n",
        "\n",
        "        mlp_out = self.mlp(self.norm2(x))\n",
        "        x = x + mlp_out\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# the Transformer class\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(args.vocab_size, args.dim, padding_idx=args.pad_token_id)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(args) for _ in range(args.n_layers)])\n",
        "        self.norm = RMSNorm(args.dim, args.norm_eps)\n",
        "        self.head = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Train_model function\n",
        "def train_model(model, train_loader, eval_loader, train_args, tokenizer):\n",
        "    model = model.to(train_args.device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: min(1.0, step / train_args.warmup_steps))\n",
        "\n",
        "    for epoch in range(train_args.n_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_loader):\n",
        "                input_ids, labels = batch\n",
        "                input_ids, labels = input_ids.to(train_args.device), labels.to(train_args.device)\n",
        "\n",
        "                outputs= model(input_ids)\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                if step % train_args.log_interval == 0:\n",
        "                    print(f\"Epoch: {epoch}, Step: {step}, Loss: {loss.item()}\")\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(eval_loader):\n",
        "                input_ids, labels = batch\n",
        "\n",
        "                input_ids, labels = input_ids.to(train_args.device), labels.to(train_args.device)\n",
        "                outputs= model(input_ids)\n",
        "\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "                eval_loss += loss.item()\n",
        "        print(f\"Epoch: {epoch}, Evaluation Loss: {eval_loss / len(eval_loader)}\")\n",
        "\n",
        "        # Save the trained model\n",
        "        model_save_path = \"llama2.pt\"\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "t_T5_Y2GNk1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.texts = f.readlines()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True,\n",
        "                                  padding='max_length',\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors='pt') # Ensure PyTorch Tensor output\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "\n",
        "        # Assuming you want to use the input_ids as labels for language modeling\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        labels[:-1] = input_ids[1:]  # Shift labels\n",
        "        return input_ids, labels  # Return both input_ids and labels\n",
        "\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if new_tokenizer.pad_token is None:\n",
        "    new_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Ensure pad_token_id is valid\n",
        "if new_tokenizer.pad_token_id is None or new_tokenizer.pad_token_id >= new_tokenizer.vocab_size:\n",
        "    new_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    new_tokenizer.pad_token_id = new_tokenizer.vocab_size - 1  # Use the last valid token ID as padding\n",
        "\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = TextDataset('/content/cleaned_data.txt', new_tokenizer, max_length=512)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "eval_dataset = TextDataset('/content/validation.txt', new_tokenizer, max_length=512)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize llama arguments\n",
        "model_args = ModelArgs(\n",
        "    dim=1024,\n",
        "    n_layers=4,\n",
        "    n_heads=4,\n",
        "    n_kv_heads=4,\n",
        "    vocab_size=new_tokenizer.vocab_size,\n",
        "    ffn_dim_multiplier=4,\n",
        "    norm_eps=1e-5,\n",
        "    batch_size=2,\n",
        "    max_seq_length=512,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    pad_token_id=new_tokenizer.pad_token_id  # Ensure this value is within the vocabulary size\n",
        ")\n",
        "\n",
        "# Initialize training arguments\n",
        "train_args = TrainArgs(\n",
        "    n_epochs=1,# Number of epochs to train the model\n",
        "    log_interval=12,# How often to log the training progress\n",
        "    lr=3e-4,        # Learning rate for the optimizer\n",
        "    warmup_steps=4000,# Number of warmup steps for the learning rate scheduler\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',# Compute device\n",
        "    vocab_size=new_tokenizer.vocab_size # Size of the model's vocabulary\n",
        ")\n",
        "# Initialize model\n",
        "model = Transformer(model_args)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, eval_loader, train_args, new_tokenizer)\n"
      ],
      "metadata": {
        "id": "rLImgo2_a5F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum length of the generated text to 30\n",
        "max_length = 30\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Set the number of sequences to generate to 5\n",
        "num_return_sequences = 5\n",
        "\n",
        "tokens = new_tokenizer.encode(\"I'm a \")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "# Add a batch dimension to the input tensor and repeat it num_return_sequences times to generate multiple sequences\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "# Move the input tensor to the device (GPU if available, otherwise CPU)\n",
        "x = tokens.to(device)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = new_tokenizer.decode(tokens)\n",
        "    print(\">\", decoded)"
      ],
      "metadata": {
        "id": "4F_GTwhreaEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}