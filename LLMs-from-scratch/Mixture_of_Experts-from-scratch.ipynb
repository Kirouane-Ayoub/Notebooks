{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "fmA_moVCVJJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.texts = f.readlines()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True,\n",
        "                                  padding='max_length',\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors='pt') # Ensure PyTorch Tensor output\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "\n",
        "        # Assuming you want to use the input_ids as labels for language modeling\n",
        "\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        labels[:-1] = input_ids[1:]  # Shift labels\n",
        "        return input_ids, labels  # Return both input_ids and labels\n",
        "\n",
        "# Function to get training data\n",
        "def get_training_corpus():\n",
        "    dataset = load_dataset(\"text\", data_files={\"train\": \"/content/cleaned_data.txt\"})\n",
        "    for i in range(0, len(dataset[\"train\"]), 1000):\n",
        "        yield dataset[\"train\"][i : i + 1000][\"text\"]\n",
        "\n",
        "\n",
        "# Load the base tokenizer\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-v0.3-bnb-4bit\")\n",
        "\n",
        "# Train the new tokenizer\n",
        "new_tokenizer = base_tokenizer.train_new_from_iterator(get_training_corpus(), vocab_size=1000)\n",
        "\n",
        "# Save the new tokenizer\n",
        "new_tokenizer.save_pretrained(\"new_tokenizer\")\n",
        "\n",
        "# Test the new tokenizer\n",
        "test_text = \"The First day \"\n",
        "encoded = new_tokenizer.encode(test_text)\n",
        "decoded = new_tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n",
        "\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if new_tokenizer.pad_token is None:\n",
        "    new_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Ensure pad_token_id is valid\n",
        "if new_tokenizer.pad_token_id is None or new_tokenizer.pad_token_id >= new_tokenizer.vocab_size:\n",
        "    new_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    new_tokenizer.pad_token_id = new_tokenizer.vocab_size - 1  # Use the last valid token ID as padding"
      ],
      "metadata": {
        "id": "48gz6Yi-VA_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuration parameters\n",
        "# n_embd = 4096\n",
        "# n_head = 32\n",
        "# n_layer = 32\n",
        "# head_size = 128\n",
        "# dropout = 0.1\n",
        "# block_size = 32768\n",
        "# vocab_size = new_tokenizer.vocab_size\n",
        "# num_experts = 8\n",
        "# top_k = 2\n",
        "\n",
        "\n",
        "\n",
        "n_embd = 768\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "head_size = 128\n",
        "dropout = 0.1\n",
        "block_size = 512\n",
        "vocab_size = new_tokenizer.vocab_size\n",
        "num_experts = 2\n",
        "top_k = 2\n",
        "# Device configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Causal scaled dot product self-Attention Head\n",
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size: int):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "# Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, head_size: int):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "# Expert module\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"An MLP representing each expert\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "# Router module\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, n_embed: int, num_experts: int, top_k: int):\n",
        "        super().__init__()\n",
        "        self.top_k = top_k\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output: torch.Tensor):\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n",
        "\n",
        "# Sparse Mixture of Experts (SMoE)\n",
        "class MoE(nn.Module):\n",
        "    def __init__(self, n_embed: int, num_experts: int, top_k: int):\n",
        "        super().__init__()\n",
        "        self.router = Router(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# Transformer Block with Multi-Head Attention and SMoE\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Mixture of Experts Transformer block\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed: int, n_head: int, num_experts: int, top_k: int):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.smoe = MoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.smoe(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Sparse MoE Language Model\n",
        "class SMoELanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head, num_experts, top_k) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embd)\n",
        "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
        "        x = self.blocks(x)  # (B, T, n_embd)\n",
        "        x = self.ln_f(x)  # (B, T, n_embd)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Instantiate the model\n",
        "model = SMoELanguageModel().to(device)\n"
      ],
      "metadata": {
        "id": "1vYtbgZLT9Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: SMoELanguageModel,\n",
        "          train_data: DataLoader,\n",
        "          val_data: DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          total_steps: int = 10,\n",
        "          device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "          clip_grad_norm: float = 1.0,\n",
        "          lr_scheduler=None,\n",
        "          eval_interval: int = 1,\n",
        "          save_path: str = \"model.pt\"):\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(\"Training...\")\n",
        "    step = 0\n",
        "    total_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while step < total_steps:\n",
        "        for batch in train_data:\n",
        "            input_ids, labels = batch\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step(loss.detach().item())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            step += 1\n",
        "\n",
        "            # Print step-wise loss and elapsed time periodically\n",
        "            if step % eval_interval == 0:\n",
        "                avg_loss = total_loss / eval_interval\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Step {step}/{total_steps} | Average loss: {avg_loss:.4f} | Elapsed time: {elapsed_time:.2f}s\")\n",
        "                total_loss = 0.0\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Evaluation Phase\n",
        "                model.eval()\n",
        "                eval_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for val_batch in val_data:\n",
        "                        input_ids, labels = val_batch\n",
        "                        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "                        logits, loss = model(input_ids, labels)\n",
        "                        eval_loss += loss.item()\n",
        "\n",
        "                avg_eval_loss = eval_loss / len(val_data)\n",
        "                print(f\"Step {step}, Evaluation Loss: {avg_eval_loss:.4f}\")\n",
        "                model.train()\n",
        "\n",
        "            # Stop if total steps reached\n",
        "            if step >= total_steps:\n",
        "                break\n",
        "\n",
        "        # Early stop if steps are already reached within epoch\n",
        "        if step >= total_steps:\n",
        "            break\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "MWriJAayU896"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
      ],
      "metadata": {
        "id": "oChC9MqDU87b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset('/content/cleaned_data.txt',\n",
        "                            new_tokenizer,\n",
        "                            max_length=512)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=4,\n",
        "                          shuffle=False)\n",
        "\n",
        "val_dataset = TextDataset('/content/validation.txt',\n",
        "                            new_tokenizer,\n",
        "                            max_length=512)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=4,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "v2pCKS-TU849"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      optimizer)"
      ],
      "metadata": {
        "id": "hKEkLmcaU82c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "num_return_sequences = 10\n",
        "\n",
        "# Encode the input text\n",
        "tokens = new_tokenizer.encode(\"One day \")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "# Generate sequences\n",
        "ids = model.generate(tokens,max_length)\n",
        "\n",
        "# Print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    decoded = new_tokenizer.decode(ids[i], skip_special_tokens=True)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "id": "VWGRq5fiu7GN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}