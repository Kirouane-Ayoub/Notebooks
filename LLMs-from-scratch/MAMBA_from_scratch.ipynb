{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "895755be6ed44824a36df2833f6d33bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d616dc56e094a6eb9de11ee510fb0a8",
              "IPY_MODEL_43737af935d240eea1b4ef8c1aa500a1",
              "IPY_MODEL_14bbc66c76914932803f3015ee48acb9"
            ],
            "layout": "IPY_MODEL_19f90540735a4473974ec4dca4f4c19e"
          }
        },
        "3d616dc56e094a6eb9de11ee510fb0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b558c47bf1d43fbbf7399f2e52f340e",
            "placeholder": "​",
            "style": "IPY_MODEL_4360fe259151488aa3492648f7592f2e",
            "value": "Generating train split: "
          }
        },
        "43737af935d240eea1b4ef8c1aa500a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e51bafcd5d3429f8b1943cf20d9c580",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54cb21b1c8a54399bb2ed83398d11717",
            "value": 1
          }
        },
        "14bbc66c76914932803f3015ee48acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_789f29ffa2b841f78fbc8ba07477aedc",
            "placeholder": "​",
            "style": "IPY_MODEL_109ba5b41cd145b7b2296f2871302066",
            "value": " 3011/0 [00:00&lt;00:00, 29721.49 examples/s]"
          }
        },
        "19f90540735a4473974ec4dca4f4c19e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b558c47bf1d43fbbf7399f2e52f340e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4360fe259151488aa3492648f7592f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e51bafcd5d3429f8b1943cf20d9c580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "54cb21b1c8a54399bb2ed83398d11717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "789f29ffa2b841f78fbc8ba07477aedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109ba5b41cd145b7b2296f2871302066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install einops datasets"
      ],
      "metadata": {
        "id": "dNpuqxM2Zheq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vS27n68ZZTk"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    vocab_size: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_conv: int = 4\n",
        "    pad_vocab_size_multiple: int = 8\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
        "            self.vocab_size += (self.pad_vocab_size_multiple\n",
        "                                - self.vocab_size % self.pad_vocab_size_multiple)"
      ],
      "metadata": {
        "id": "8afrjv0UZryY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        \"\"\"Full Mamba model.\"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
        "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
        "        self.norm_f = RMSNorm(args.d_model)\n",
        "\n",
        "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
        "                                                     # See \"Weight Tying\" paper\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, RMSNorm):\n",
        "            nn.init.ones_(module.weight)\n",
        "        elif isinstance(module, nn.MultiheadAttention):\n",
        "            nn.init.xavier_uniform_(module.in_proj_weight)\n",
        "            if module.in_proj_bias is not None:\n",
        "                nn.init.zeros_(module.in_proj_bias)\n",
        "            nn.init.xavier_uniform_(module.out_proj.weight)\n",
        "            if module.out_proj.bias is not None:\n",
        "                nn.init.zeros_(module.out_proj.bias)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pretrained(pretrained_model_name: str):\n",
        "        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
        "        from transformers.utils.hub import cached_file\n",
        "\n",
        "        def load_config_hf(model_name):\n",
        "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
        "                                                _raise_exceptions_for_missing_entries=False)\n",
        "            return json.load(open(resolved_archive_file))\n",
        "\n",
        "\n",
        "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
        "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
        "                                                _raise_exceptions_for_missing_entries=False)\n",
        "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
        "\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        args = ModelArgs(\n",
        "            d_model=config_data['d_model'],\n",
        "            n_layer=config_data['n_layer'],\n",
        "            vocab_size=config_data['vocab_size']\n",
        "        )\n",
        "        model = Mamba(args)\n",
        "\n",
        "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
        "        new_state_dict = {}\n",
        "        for key in state_dict:\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            new_state_dict[new_key] = state_dict[key]\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "gOdo1TIhZxAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "mXV2JexSaD_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
        "\n",
        "        # dt_proj projects Δ from dt_rank to d_in\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
        "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "\n",
        "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def ssm(self, x):\n",
        "\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)\n",
        "\n",
        "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)\n",
        "        delta = F.softplus(self.dt_proj(delta))\n",
        "\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "\n",
        "\n",
        "        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n",
        "        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "\n",
        "\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "\n",
        "        y = y + u * D\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "DdXP9-yjaXuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "hxEJEzSFajRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Random weights\n",
        "\n"
      ],
      "metadata": {
        "id": "OAdrh0MbBqkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "mamba-370m :\n",
        "\n",
        "        d_model: 1024\n",
        "        n_layer: 48\n",
        "        vocab_size: 50280\n",
        "        d_state: 4096\n",
        "        expand: 4\n",
        "        d_conv: 4\n",
        "\n",
        "\n",
        "\n",
        "mamba-130m :\n",
        "\n",
        "        d_model: 768\n",
        "        n_layer: 24\n",
        "        vocab_size: 50280\n",
        "        d_state: 3072\n",
        "        expand: 4\n",
        "        d_conv: 4\n",
        "\"\"\"\n",
        "\n",
        "args = ModelArgs(\n",
        "    d_model=768,            # Hidden dimension size\n",
        "    n_layer=24,             # Number of layers\n",
        "    vocab_size=50280,       # Vocabulary size\n",
        "    d_state=3072,           # Latent state dimension\n",
        "    expand=4,             # Expansion factor\n",
        "    dt_rank='auto',       # Rank of delta\n",
        "    d_conv=4,             # Convolution kernel size\n",
        "    pad_vocab_size_multiple=8,\n",
        "    conv_bias=True,\n",
        "    bias=False\n",
        ")\n",
        "model = Mamba(args)"
      ],
      "metadata": {
        "id": "7eOIryMaa5UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "mONYh2mNj2q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a92adf2-8c60-4bcb-ad21-b6e2e7e75265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mamba(\n",
              "  (embedding): Embedding(50280, 768)\n",
              "  (layers): ModuleList(\n",
              "    (0-23): 24 x ResidualBlock(\n",
              "      (mixer): MambaBlock(\n",
              "        (in_proj): Linear(in_features=768, out_features=6144, bias=False)\n",
              "        (conv1d): Conv1d(3072, 3072, kernel_size=(4,), stride=(1,), padding=(3,), groups=3072)\n",
              "        (x_proj): Linear(in_features=3072, out_features=6192, bias=False)\n",
              "        (dt_proj): Linear(in_features=48, out_features=3072, bias=True)\n",
              "        (out_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
              "      )\n",
              "      (norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm_f): RMSNorm()\n",
              "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stTR3Wq8WWRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FROM PRE-TRAIN MODEL"
      ],
      "metadata": {
        "id": "Al4Yhcc_Bt9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name = 'state-spaces/mamba-130m'\n",
        "model = Mamba.from_pretrained(pretrained_model_name)"
      ],
      "metadata": {
        "id": "uPCgr6rVbWP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEsbSiQKcPF3",
        "outputId": "9536ab1e-6295-4b34-80fb-085b9bfe98ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mamba(\n",
              "  (embedding): Embedding(50280, 768)\n",
              "  (layers): ModuleList(\n",
              "    (0-23): 24 x ResidualBlock(\n",
              "      (mixer): MambaBlock(\n",
              "        (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
              "        (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
              "        (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
              "        (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
              "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
              "      )\n",
              "      (norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm_f): RMSNorm()\n",
              "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59fkCXhhCCP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "CFWMkjqoCCvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Function to get training data\n",
        "def get_training_corpus():\n",
        "    dataset = load_dataset(\"text\", data_files={\"train\": \"cleaned_data.txt\"})\n",
        "    for i in range(0, len(dataset[\"train\"]), 1000):\n",
        "        yield dataset[\"train\"][i : i + 1000][\"text\"]\n",
        "\n",
        "# Initialize the base tokenizer\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
        "\n",
        "# Train the new tokenizer\n",
        "new_tokenizer = base_tokenizer.train_new_from_iterator(get_training_corpus(), vocab_size=1000)\n",
        "\n",
        "# Save the new tokenizer\n",
        "new_tokenizer.save_pretrained(\"darija_tokenizer\")\n",
        "\n",
        "new_tokenizer.pad_token = new_tokenizer.eos_token\n",
        "\n",
        "fim_prefix_token = \"<fim_prefix>\"\n",
        "fim_middle_token = \"<fim_middle_token>\"\n",
        "fim_suffix_token = \"<fim_suffix_token>\"\n",
        "fim_pad_token = \"<fim_pad>\"\n",
        "\n",
        "# Get the FIM-specific tokens and get their token ids\n",
        "new_tokenizer.add_tokens(\n",
        "    [\n",
        "        fim_prefix_token,\n",
        "        fim_middle_token,\n",
        "        fim_middle_token,\n",
        "        fim_pad_token,\n",
        "    ]\n",
        ")\n",
        "prefix_tok_id = new_tokenizer.convert_tokens_to_ids(fim_prefix_token)\n",
        "middle_tok_id = new_tokenizer.convert_tokens_to_ids(fim_middle_token)\n",
        "suffix_tok_id = new_tokenizer.convert_tokens_to_ids(fim_middle_token)\n",
        "pad_tok_id = None\n",
        "\n",
        "fim_tokens = [prefix_tok_id, middle_tok_id, suffix_tok_id]\n",
        "\n",
        "\n",
        "# If truncate_or_pad is on, also get pad token id\n",
        "truncate_or_pad = True\n",
        "if truncate_or_pad:\n",
        "    pad_tok_id = new_tokenizer.convert_tokens_to_ids(fim_pad_token)\n",
        "    fim_tokens.append(pad_tok_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "895755be6ed44824a36df2833f6d33bc",
            "3d616dc56e094a6eb9de11ee510fb0a8",
            "43737af935d240eea1b4ef8c1aa500a1",
            "14bbc66c76914932803f3015ee48acb9",
            "19f90540735a4473974ec4dca4f4c19e",
            "9b558c47bf1d43fbbf7399f2e52f340e",
            "4360fe259151488aa3492648f7592f2e",
            "2e51bafcd5d3429f8b1943cf20d9c580",
            "54cb21b1c8a54399bb2ed83398d11717",
            "789f29ffa2b841f78fbc8ba07477aedc",
            "109ba5b41cd145b7b2296f2871302066"
          ]
        },
        "id": "2zObYsgTMkRp",
        "outputId": "9d1839b8-2eb8-4831-e5ec-8cc2fbeff41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "895755be6ed44824a36df2833f6d33bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextDataset"
      ],
      "metadata": {
        "id": "gSV7HKINCF10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, context_len=384):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # Load and tokenize data\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = f.read()\n",
        "\n",
        "        self.tokens = tokenizer(self.data, return_tensors='pt', truncation=True, padding='max_length', max_length=context_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokens['input_ids'][idx],\n",
        "            'labels': self.tokens['input_ids'][idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "DwthJaYjf_gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training script\n",
        "class Args:\n",
        "    dataset_path = \"/content/cleaned_data.txt\"  # Path to your raw text file\n",
        "    eval_path = \"/content/validation.txt\"\n",
        "    lr = 1e-4\n",
        "    epochs = 1\n",
        "    context_len = 384\n",
        "    train_batch_size = 8\n",
        "    valid_batch_size = 8\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mo5b7NVthovM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_dataset = TextDataset(Args.dataset_path, new_tokenizer, context_len=Args.context_len)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=Args.train_batch_size, shuffle=True)\n",
        "\n",
        "eval_dataset = TextDataset(Args.eval_path, new_tokenizer, context_len=Args.context_len)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=Args.valid_batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=Args.lr)\n",
        "scheduler = get_scheduler(\n",
        "    \"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * Args.epochs\n",
        ")"
      ],
      "metadata": {
        "id": "nU4NS3GBhj6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "IqPfvSjLCI5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(Args.device)\n",
        "for epoch in range(Args.epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(Args.device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(batch['input_ids'])\n",
        "\n",
        "        # Compute the loss manually\n",
        "        shift_logits = outputs[..., :-1, :].contiguous()\n",
        "        shift_labels = batch['labels'][..., 1:].contiguous()\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{Args.epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            batch = {k: v.to(Args.device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(batch['input_ids'])\n",
        "\n",
        "            # Compute the loss manually\n",
        "            shift_logits = outputs[..., :-1, :].contiguous()\n",
        "            shift_labels = batch['labels'][..., 1:].contiguous()\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{Args.epochs}, Evaluation Loss: {avg_eval_loss}\")\n",
        "    model_save_path = \"MAMBA_MODEL.pt\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "IqneJbTyiU3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "vXzLOFmsCMj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "num_return_sequences = 10\n",
        "\n",
        "\n",
        "tokens = new_tokenizer.encode(\"You are  \")\n",
        "tokens = torch.tensor(tokens , dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "x = tokens.to(Args.device)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "        logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = new_tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    print(\">\", decoded)"
      ],
      "metadata": {
        "id": "1rVWAvFLlALk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}